{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bbc793b",
   "metadata": {},
   "source": [
    "# 8. Model Training\n",
    "\n",
    "Train and optimize multiple machine learning models using the selected features. Compare different algorithms and hyperparameters to find the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81727fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model Training Setup\n",
    "print(\"ü§ñ Model Training Setup\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "available_features = [f for f in manual_selected_features if f in X_train.columns]\n",
    "    \n",
    "print(f\"Using top correlation features: {available_features[:5]}...\")  # Show first 5\n",
    "print(f\"Selected {len(available_features)} features from top importance analysis:\")\n",
    "for i, feature in enumerate(available_features, 1):\n",
    "    print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "# Prepare training data with selected features\n",
    "X_train_selected = X_train[available_features].copy()\n",
    "X_test_selected = X_test[available_features].copy()\n",
    "\n",
    "print(f\"\\nüìä Training Data Preparation:\")\n",
    "print(f\"  ‚Ä¢ Selected features: {len(available_features)}\")\n",
    "print(f\"  ‚Ä¢ Training samples: {len(X_train_selected):,}\")\n",
    "print(f\"  ‚Ä¢ Test samples: {len(X_test_selected):,}\")\n",
    "print(f\"  ‚Ä¢ Feature reduction: {((len(X_train.columns) - len(available_features)) / len(X_train.columns) * 100):.1f}%\")\n",
    "\n",
    "print(f\"\\n‚úÖ Model training setup complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f940afc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train Multiple Models with Optimized Parameters\n",
    "print(\"üèãÔ∏è Training Multiple Models with Enhanced Configuration\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Advanced models for better noise handling\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# Create feature scaler for models that benefit from scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Define models to train with optimized parameters\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'K-Neighbors': KNeighborsRegressor(n_neighbors=5),\n",
    "    # 'Support Vector': SVR(kernel='rbf', C=1.0, epsilon=0.01),\n",
    "    \n",
    "    # Optimized XGBoost (reduced overfitting)\n",
    "    'XGBoost (Tuned)': xgb.XGBRegressor(\n",
    "        n_estimators=50,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    ),\n",
    "    \n",
    "    # Optimized CatBoost\n",
    "    'CatBoost (Tuned)': cb.CatBoostRegressor(\n",
    "        iterations=75,\n",
    "        learning_rate=0.08,\n",
    "        depth=4,\n",
    "        l2_leaf_reg=3,\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    ),\n",
    "    \n",
    "    'Huber Regressor': HuberRegressor(epsilon=1.35, alpha=0.001),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=50, max_depth=8, min_samples_split=10, random_state=42),\n",
    "    \n",
    "}\n",
    "\n",
    "# Train models and store results\n",
    "model_results = []\n",
    "\n",
    "print(\"Training and evaluating models...\")\n",
    "print(f\"{\"Model\":<22} {\"Train R¬≤\":<10} {\"Test R¬≤\":<10} {\"Train RMSE\":<12} {\"Test RMSE\":<12} {\"Time (s)\":<10}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Train regular models\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    train_pred = model.predict(X_train_selected)\n",
    "    test_pred = model.predict(X_test_selected)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    model_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Train R¬≤\": train_r2,\n",
    "        \"Test R¬≤\": test_r2,\n",
    "        \"Train RMSE\": train_rmse,\n",
    "        \"Test RMSE\": test_rmse,\n",
    "        \"Training Time\": training_time,\n",
    "        \"Model Object\": model\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:<22} {train_r2:<10.4f} {test_r2:<10.4f} {train_rmse:<12.4f} {test_rmse:<12.4f} {training_time:<10.2f}\")\n",
    "\n",
    "# Train scaled versions\n",
    "scaled_models = {\n",
    "    \"K-Neighbors (Scaled)\": KNeighborsRegressor(n_neighbors=5),\n",
    "   # \"Support Vector (Scaled)\": SVR(kernel=\"rbf\", C=1.0, epsilon=0.01)\n",
    "}\n",
    "\n",
    "print(f\"\\n{\"Scaled Models\":<22} {\"Train R¬≤\":<10} {\"Test R¬≤\":<10} {\"Train RMSE\":<12} {\"Test RMSE\":<12} {\"Time (s)\":<10}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for name, model in scaled_models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_pred = model.predict(X_train_scaled)\n",
    "    test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    model_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Train R¬≤\": train_r2,\n",
    "        \"Test R¬≤\": test_r2,\n",
    "        \"Train RMSE\": train_rmse,\n",
    "        \"Test RMSE\": test_rmse,\n",
    "        \"Training Time\": training_time,\n",
    "        \"Model Object\": model\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:<22} {train_r2:<10.4f} {test_r2:<10.4f} {train_rmse:<12.4f} {test_rmse:<12.4f} {training_time:<10.2f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All models trained successfully\")\n",
    "\n",
    "# Sort by Test R¬≤ score\n",
    "model_results_df = pd.DataFrame(model_results)\n",
    "model_results_df = model_results_df.sort_values(\"Test R¬≤\", ascending=False)\n",
    "\n",
    "print(f\"\\nModel training complete - {len(model_results)} models evaluated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce119beb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model Performance Analysis\n",
    "print(\"üìä Model Performance Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Add overfitting calculation\n",
    "model_results_df[\"Overfitting\"] = model_results_df[\"Train R¬≤\"] - model_results_df[\"Test R¬≤\"]\n",
    "\n",
    "# Sort by test R¬≤ score\n",
    "results_sorted = model_results_df.sort_values(\"Test R¬≤\", ascending=False)\n",
    "\n",
    "print(\"üèÜ MODEL LEADERBOARD (by Test R¬≤):\")\n",
    "print(f\"{\"Rank\":<4} {\"Model\":<18} {\"Test R¬≤\":<10} {\"Test RMSE\":<12} {\"Overfitting\":<12} {\"Time (s)\":<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, (_, row) in enumerate(results_sorted.iterrows(), 1):\n",
    "    print(f\"{i:<4} {row[\"Model\"]:<18} {row[\"Test R¬≤\"]:<10.4f} {row[\"Test RMSE\"]:<12.4f} {row[\"Overfitting\"]:<12.4f} {row[\"Training Time\"]:<10.2f}\")\n",
    "\n",
    "# Identify best models\n",
    "best_model = results_sorted.iloc[0]\n",
    "least_overfit = model_results_df.loc[model_results_df[\"Overfitting\"].abs().idxmin()]\n",
    "fastest_model = model_results_df.loc[model_results_df[\"Training Time\"].idxmin()]\n",
    "\n",
    "print(f\"\\nüéØ KEY FINDINGS:\")\n",
    "print(f\"  ‚Ä¢ Best Performance: {best_model[\"Model\"]} (R¬≤ = {best_model[\"Test R¬≤\"]:.4f})\")\n",
    "print(f\"  ‚Ä¢ Least Overfitting: {least_overfit[\"Model\"]} (Œî = {least_overfit[\"Overfitting\"]:.4f})\")\n",
    "print(f\"  ‚Ä¢ Fastest Training: {fastest_model[\"Model\"]} ({fastest_model[\"Training Time\"]:.2f}s)\")\n",
    "\n",
    "# Performance categories\n",
    "excellent_models = model_results_df[model_results_df[\"Test R¬≤\"] > 0.9]\n",
    "good_models = model_results_df[(model_results_df[\"Test R¬≤\"] > 0.8) & (model_results_df[\"Test R¬≤\"] <= 0.9)]\n",
    "fair_models = model_results_df[(model_results_df[\"Test R¬≤\"] > 0.6) & (model_results_df[\"Test R¬≤\"] <= 0.8)]\n",
    "\n",
    "print(f\"\\nüìà PERFORMANCE CATEGORIES:\")\n",
    "print(f\"  ‚Ä¢ Excellent (R¬≤ > 0.9): {len(excellent_models)} models\")\n",
    "if len(excellent_models) > 0:\n",
    "    print(f\"    - {\", \".join(excellent_models[\"Model\"].tolist())}\")\n",
    "print(f\"  ‚Ä¢ Good (R¬≤ 0.8-0.9): {len(good_models)} models\")\n",
    "if len(good_models) > 0:\n",
    "    print(f\"    - {\", \".join(good_models[\"Model\"].tolist())}\")\n",
    "print(f\"  ‚Ä¢ Fair (R¬≤ 0.6-0.8): {len(fair_models)} models\")\n",
    "if len(fair_models) > 0:\n",
    "    print(f\"    - {\", \".join(fair_models[\"Model\"].tolist())}\")\n",
    "\n",
    "# Best model details\n",
    "print(f\"\\nüèÖ RECOMMENDED MODEL: {best_model[\"Model\"]}\")\n",
    "print(f\"  ‚Ä¢ Training R¬≤: {best_model[\"Train R¬≤\"]:.4f}\")\n",
    "print(f\"  ‚Ä¢ Test R¬≤: {best_model[\"Test R¬≤\"]:.4f}\")\n",
    "print(f\"  ‚Ä¢ Test RMSE: {best_model[\"Test RMSE\"]:.4f}\")\n",
    "print(f\"  ‚Ä¢ Train RMSE: {best_model[\"Train RMSE\"]:.4f}\")\n",
    "print(f\"  ‚Ä¢ Overfitting: {best_model[\"Overfitting\"]:.4f}\")\n",
    "print(f\"  ‚Ä¢ Training Time: {best_model[\"Training Time\"]:.2f}s\")\n",
    "\n",
    "# Store best model for evaluation section\n",
    "best_trained_model = best_model[\"Model Object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e32a01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize Model Performance\n",
    "print(\"üìà Model Performance Visualization\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. R¬≤ Score Comparison\n",
    "models_list = model_results_df['Model'].tolist()\n",
    "train_r2 = model_results_df['Train R¬≤'].tolist()\n",
    "test_r2 = model_results_df['Test R¬≤'].tolist()\n",
    "\n",
    "x = np.arange(len(models_list))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, train_r2, width, label='Train R¬≤', alpha=0.8)\n",
    "ax1.bar(x + width/2, test_r2, width, label='Test R¬≤', alpha=0.8)\n",
    "ax1.set_xlabel('Models')\n",
    "ax1.set_ylabel('R¬≤ Score')\n",
    "ax1.set_title('R¬≤ Score: Train vs Test')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. RMSE Comparison\n",
    "train_rmse = model_results_df['Train RMSE'].tolist()\n",
    "test_rmse = model_results_df['Test RMSE'].tolist()\n",
    "\n",
    "ax2.bar(x - width/2, train_rmse, width, label='Train RMSE', alpha=0.8)\n",
    "ax2.bar(x + width/2, test_rmse, width, label='Test RMSE', alpha=0.8)\n",
    "ax2.set_xlabel('Models')\n",
    "ax2.set_ylabel('RMSE')\n",
    "ax2.set_title('RMSE: Train vs Test')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Overfitting Analysis\n",
    "overfitting = model_results_df['Overfitting'].tolist()\n",
    "colors = ['green' if x <= 0.1 else 'orange' if x <= 0.2 else 'red' for x in overfitting]\n",
    "\n",
    "bars = ax3.bar(models_list, overfitting, color=colors, alpha=0.7)\n",
    "ax3.set_xlabel('Models')\n",
    "ax3.set_ylabel('Overfitting (Train R¬≤ - Test R¬≤)')\n",
    "ax3.set_title('Overfitting Analysis')\n",
    "ax3.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "ax3.axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, label='Moderate (0.1)')\n",
    "ax3.axhline(y=0.2, color='red', linestyle='--', alpha=0.7, label='High (0.2)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Performance vs Training Time\n",
    "training_times = model_results_df['Training Time'].tolist()\n",
    "test_r2_scores = model_results_df['Test R¬≤'].tolist()\n",
    "\n",
    "scatter = ax4.scatter(training_times, test_r2_scores, s=100, alpha=0.7)\n",
    "for i, model in enumerate(models_list):\n",
    "    ax4.annotate(model[:8], (training_times[i], test_r2_scores[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "ax4.set_xlabel('Training Time (seconds)')\n",
    "ax4.set_ylabel('Test R¬≤ Score')\n",
    "ax4.set_title('Performance vs Training Time')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä SUMMARY STATISTICS:\")\n",
    "print(f\"  ‚Ä¢ Average Test R¬≤: {model_results_df['Test R¬≤'].mean():.4f}\")\n",
    "print(f\"  ‚Ä¢ Best Test R¬≤: {model_results_df['Test R¬≤'].max():.4f}\")\n",
    "print(f\"  ‚Ä¢ Average RMSE: {model_results_df['Test RMSE'].mean():.4f}\")\n",
    "print(f\"  ‚Ä¢ Best RMSE: {model_results_df['Test RMSE'].min():.4f}\")\n",
    "print(f\"  ‚Ä¢ Average Training Time: {model_results_df['Training Time'].mean():.2f}s\")\n",
    "print(f\"  ‚Ä¢ Models with R¬≤ > 0.8: {len(model_results_df[model_results_df['Test R¬≤'] > 0.8])}/{len(model_results_df)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Model training visualization complete\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
