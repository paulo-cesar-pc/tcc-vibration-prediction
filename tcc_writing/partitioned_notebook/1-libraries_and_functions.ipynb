{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5026ac8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "**Purpose**: Load all required dependencies for data processing and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35d862a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f2b8d5",
   "metadata": {},
   "source": [
    "## 2. Functions\n",
    "**Purpose**: Define core functions for data processing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b584c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Core functions defined successfully\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_path='full_data/'):\n",
    "    \"\"\"\n",
    "    Load and combine all CSV files from the data directory\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Combined industrial data with timestamp index\n",
    "    \"\"\"\n",
    "    csv_files = sorted(glob.glob(os.path.join(data_path, '*.csv')))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {data_path}\")\n",
    "    \n",
    "    print(f\"ðŸ“ Loading {len(csv_files)} CSV files...\")\n",
    "    \n",
    "    # Load and combine all files\n",
    "    df_list = []\n",
    "    for file in csv_files:\n",
    "        df_temp = pd.read_csv(file)\n",
    "        df_list.append(df_temp)\n",
    "    \n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Parse timestamp and set as index\n",
    "    df['Timestamps'] = pd.to_datetime(df['Timestamps'], format='%d/%m/%Y %H:%M:%S')\n",
    "    df = df.set_index('Timestamps').sort_index()\n",
    "    df.drop(columns='CM2_PV_VRM01_VIBRATION1', axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data(df, remove_outliers=False, outlier_columns=None):\n",
    "    \"\"\"\n",
    "    Clean industrial data by removing invalid values and handling missing data\n",
    "    \n",
    "    Args:\n",
    "        df: Raw industrial DataFrame\n",
    "        remove_outliers: Boolean flag to remove outliers using percentile method (default: False)\n",
    "        outlier_columns: List of columns to apply outlier removal (default: None = all numeric columns)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (cleaned_df, target_column)\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Identify vibration target column\n",
    "    vibration_cols = [col for col in df_clean.columns if 'VIBRATION' in col.upper()]\n",
    "    if not vibration_cols:\n",
    "        raise ValueError(\"No vibration column found in data\")\n",
    "    \n",
    "    target_col = vibration_cols[0]  # Use first vibration column as target\n",
    "    \n",
    "    # Filter realistic vibration values (0-50 mm/s is reasonable for industrial mills)\n",
    "    initial_len = len(df_clean)\n",
    "    df_clean = df_clean[(df_clean[target_col] > 0) & (df_clean[target_col] <= 12)]\n",
    "    filtered_count = initial_len - len(df_clean)\n",
    "    \n",
    "    # Optional outlier removal using percentile method (gentle approach for industrial data)\n",
    "    outliers_removed = 0\n",
    "    if remove_outliers:\n",
    "        # Determine which columns to process\n",
    "        if outlier_columns is None:\n",
    "            # Apply to all numeric columns by default\n",
    "            columns_to_clean = df_clean.select_dtypes(include=[np.number]).columns\n",
    "        else:\n",
    "            # Apply only to specified columns\n",
    "            columns_to_clean = [col for col in outlier_columns if col in df_clean.columns]\n",
    "        \n",
    "        initial_outlier_len = len(df_clean)\n",
    "        \n",
    "        # Apply percentile-based filtering (5th-95th percentile = keep 90% of data)\n",
    "        for col in columns_to_clean:\n",
    "            lower_bound = df_clean[col].quantile(0.5)  # 5th percentile\n",
    "            upper_bound = df_clean[col].quantile(0.95)  # 95th percentile\n",
    "            \n",
    "            # Remove outliers for this column\n",
    "            df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "        \n",
    "        outliers_removed = initial_outlier_len - len(df_clean)\n",
    "        print(f\"  â€¢ Processed {len(columns_to_clean)} columns for outlier removal\")\n",
    "    \n",
    "    # Remove columns with >50% missing data\n",
    "    missing_threshold = 0.5\n",
    "    missing_ratios = df_clean.isnull().sum() / len(df_clean)\n",
    "    cols_to_drop = missing_ratios[missing_ratios > missing_threshold].index\n",
    "    df_clean = df_clean.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Fill remaining missing values\n",
    "    df_clean = df_clean.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    print(f\"ðŸ§¹ Cleaning results:\")\n",
    "    print(f\"  â€¢ Filtered {filtered_count:,} invalid vibration readings\")\n",
    "    if remove_outliers:\n",
    "        print(f\"  â€¢ Removed {outliers_removed:,} outliers using percentile method (5th-95th)\")\n",
    "    print(f\"  â€¢ Removed {len(cols_to_drop)} columns with >50% missing data\")\n",
    "    print(f\"  â€¢ Target column: {target_col}\")\n",
    "    \n",
    "    return df_clean, target_col\n",
    "\n",
    "\n",
    "def resample_aggregate(df, target_col, agg):\n",
    "    \"\"\"\n",
    "    Resample 30-second data to X-minute intervals to reduce noise\n",
    "    \n",
    "    Args:\n",
    "        df: Cleaned DataFrame with 30-second intervals\n",
    "        target_col: Name of target column\n",
    "        agg: Aggregation of resample\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Resampled DataFrame with X-minute intervals\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ“Š Resampling from 30s to {int(agg[:-1])}min intervals...\")\n",
    "    print(f\"  â€¢ Original shape: {df.shape}\")\n",
    "    \n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Define aggregation strategy for different column types\n",
    "    agg_funcs = {}\n",
    "    \n",
    "    # For most numeric columns, use mean\n",
    "    for col in numeric_cols:\n",
    "        if col == target_col:\n",
    "            # For target (vibration), use mean as primary but also capture variability\n",
    "            agg_funcs[col] = 'mean'\n",
    "        elif 'TEMPERATURE' in col.upper() or 'PRESSURE' in col.upper() or 'FLOW' in col.upper():\n",
    "            # Process variables - use mean\n",
    "            agg_funcs[col] = 'mean'\n",
    "        elif 'VIBRATION' in col.upper():\n",
    "            # Other vibration variables - use mean\n",
    "            agg_funcs[col] = 'mean'\n",
    "        else:\n",
    "            # Default to mean for other numeric columns\n",
    "            agg_funcs[col] = 'mean'\n",
    "    \n",
    "    # Perform resampling to 5-minute intervals\n",
    "    df_resampled = df.resample(agg).agg(agg_funcs)\n",
    "    \n",
    "    # Remove any rows with all NaN values (shouldn't happen with proper data)\n",
    "    df_resampled = df_resampled.dropna(how='all')\n",
    "    \n",
    "    # Forward fill any remaining NaN values, then backward fill\n",
    "    df_resampled = df_resampled.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    print(f\"  â€¢ Resampled shape: {df_resampled.shape}\")\n",
    "    print(f\"  â€¢ Data reduction: {len(df) - len(df_resampled):,} rows removed ({((len(df) - len(df_resampled))/len(df)*100):.1f}%)\")\n",
    "    print(f\"  â€¢ Time interval: {int(agg[:-1])} minutes\")\n",
    "    \n",
    "    return df_resampled\n",
    "\n",
    "\n",
    "def engineer_features(df, target_col):\n",
    "    \"\"\"\n",
    "    Create engineered features while preventing data leakage\n",
    "    \n",
    "    Args:\n",
    "        df: Cleaned DataFrame\n",
    "        target_col: Name of target column to exclude from feature engineering\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # CRITICAL: Exclude ALL vibration columns from feature engineering\n",
    "    feature_cols = [col for col in df_features.columns if 'VIBRATION' not in col.upper()]\n",
    "    \n",
    "    print(f\"ðŸ”§ Engineering features from {len(feature_cols)} non-vibration columns...\")\n",
    "    \n",
    "    # Create rolling statistics for key process variables\n",
    "    key_vars = []\n",
    "    for pattern in ['POWER', 'PRESSURE', 'CURRENT', 'FLOW', 'TEMPERATURE']:\n",
    "        pattern_cols = [col for col in feature_cols if pattern in col.upper()]\n",
    "        key_vars.extend(pattern_cols[:2])  # Take first 2 matches to avoid explosion\n",
    "    \n",
    "    # Create rolling features\n",
    "    windows = [3, 6, 12]  # 15min, 30min, 1hr for 5-minute data\n",
    "    feature_count = 0\n",
    "    \n",
    "    for col in key_vars:\n",
    "        for window in windows:\n",
    "            # Rolling mean and std\n",
    "            df_features[f\"{col}_rolling_mean_{window}\"] = df_features[col].rolling(window).mean()\n",
    "            df_features[f\"{col}_rolling_std_{window}\"] = df_features[col].rolling(window).std()\n",
    "            feature_count += 2\n",
    "    \n",
    "    # Add time-based features\n",
    "    df_features['hour'] = df_features.index.hour\n",
    "    df_features['day_of_week'] = df_features.index.dayofweek\n",
    "    df_features['month'] = df_features.index.month\n",
    "    feature_count += 3\n",
    "    \n",
    "    # Remove infinite values and excessive missing data\n",
    "    df_features = df_features.replace([np.inf, -np.inf], np.nan)\n",
    "    missing_ratios = df_features.isnull().sum() / len(df_features)\n",
    "    cols_to_drop = missing_ratios[missing_ratios > 0.7].index\n",
    "    df_features = df_features.drop(columns=cols_to_drop)\n",
    "    df_features = df_features.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    print(f\"âœ… Created {feature_count} engineered features\")\n",
    "    print(f\"ðŸ›¡ï¸ Target column '{target_col}' excluded from features\")\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "\n",
    "def prepare_model_data(df, target_col, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for modeling with proper time series splits\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with features and target\n",
    "        target_col: Name of target column\n",
    "        test_size: Proportion of data for testing\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Time series split (no shuffling - maintain temporal order)\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    \n",
    "    X_train = X.iloc[:split_idx]\n",
    "    X_test = X.iloc[split_idx:]\n",
    "    y_train = y.iloc[:split_idx]\n",
    "    y_test = y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"ðŸ“Š Data split:\")\n",
    "    print(f\"  â€¢ Training: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"  â€¢ Testing: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    print(f\"  â€¢ Features: {X_train.shape[1]}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def plot_all_time_series(df, figsize_per_plot=(12, 4), cols_per_row=3, save_path=None, show_plots=True):\n",
    "    \"\"\"\n",
    "    Create time series plots for all columns in the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing time series data\n",
    "    figsize_per_plot : tuple\n",
    "        Size of each individual subplot (width, height)\n",
    "    cols_per_row : int\n",
    "        Number of columns per row in the subplot grid\n",
    "    save_path : str, optional\n",
    "        Path to save the plots (without extension, will save as PNG)\n",
    "    show_plots : bool\n",
    "        Whether to display the plots\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get numeric columns only\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    n_cols = len(numeric_cols)\n",
    "\n",
    "    if n_cols == 0:\n",
    "        print(\"No numeric columns found in dataframe.\")\n",
    "        return\n",
    "\n",
    "    # Calculate grid dimensions\n",
    "    n_rows = ceil(n_cols / cols_per_row)\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(n_rows, cols_per_row, \n",
    "                            figsize=(figsize_per_plot[0] * cols_per_row, \n",
    "                                    figsize_per_plot[1] * n_rows))\n",
    "\n",
    "    # Ensure axes is always a 2D array\n",
    "    if n_rows == 1 and cols_per_row == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif cols_per_row == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "\n",
    "    # Plot each column\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        row = i // cols_per_row\n",
    "        col_idx = i % cols_per_row\n",
    "        ax = axes[row, col_idx]\n",
    "        \n",
    "        # Plot time series\n",
    "        ax.plot(df.index, df[col], linewidth=0.8, alpha=0.8)\n",
    "        ax.set_title(f'{col}', fontsize=10, fontweight='bold')\n",
    "        ax.set_xlabel('Time', fontsize=8)\n",
    "        ax.set_ylabel('Value', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=7)\n",
    "        \n",
    "        # Add basic statistics as text\n",
    "        stats_text = f'Mean: {df[col].mean():.3f}\\nStd: {df[col].std():.3f}'\n",
    "        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n",
    "                fontsize=7, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for i in range(n_cols, n_rows * cols_per_row):\n",
    "        row = i // cols_per_row\n",
    "        col_idx = i % cols_per_row\n",
    "        axes[row, col_idx].set_visible(False)\n",
    "\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.suptitle(f'Time Series Plots - All {n_cols} Numeric Columns', \n",
    "                    fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(f'{save_path}_timeseries.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Time series plots saved to {save_path}_timeseries.png\")\n",
    "\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"âœ… Created time series plots for {n_cols} columns\")\n",
    "\n",
    "\n",
    "def plot_all_histograms(df, figsize_per_plot=(12, 4), cols_per_row=3, bins=50, save_path=None, show_plots=True):\n",
    "    \"\"\"\n",
    "    Create histogram plots for all columns in the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing the data\n",
    "    figsize_per_plot : tuple\n",
    "        Size of each individual subplot (width, height)\n",
    "    cols_per_row : int\n",
    "        Number of columns per row in the subplot grid\n",
    "    bins : int or str\n",
    "        Number of bins for histograms\n",
    "    save_path : str, optional\n",
    "        Path to save the plots (without extension, will save as PNG)\n",
    "    show_plots : bool\n",
    "        Whether to display the plots\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get numeric columns only\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    n_cols = len(numeric_cols)\n",
    "\n",
    "    if n_cols == 0:\n",
    "        print(\"No numeric columns found in dataframe.\")\n",
    "        return\n",
    "\n",
    "    # Calculate grid dimensions\n",
    "    n_rows = ceil(n_cols / cols_per_row)\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(n_rows, cols_per_row, \n",
    "                            figsize=(figsize_per_plot[0] * cols_per_row, \n",
    "                                    figsize_per_plot[1] * n_rows))\n",
    "\n",
    "    # Ensure axes is always a 2D array\n",
    "    if n_rows == 1 and cols_per_row == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif cols_per_row == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "\n",
    "    # Plot each column\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        row = i // cols_per_row\n",
    "        col_idx = i % cols_per_row\n",
    "        ax = axes[row, col_idx]\n",
    "        \n",
    "        # Remove NaN values for plotting\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        if len(data) == 0:\n",
    "            ax.text(0.5, 0.5, 'No data available', \n",
    "                    transform=ax.transAxes, ha='center', va='center')\n",
    "            ax.set_title(f'{col}', fontsize=10, fontweight='bold')\n",
    "            continue\n",
    "        \n",
    "        # Plot histogram\n",
    "        n, bins_edges, patches = ax.hist(data, bins=bins, alpha=0.7, \n",
    "                                        edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        # Add KDE overlay if enough data points\n",
    "        if len(data) > 10:\n",
    "            try:\n",
    "                ax2 = ax.twinx()\n",
    "                data.plot.density(ax=ax2, color='red', linewidth=2, alpha=0.8)\n",
    "                ax2.set_ylabel('Density', fontsize=8, color='red')\n",
    "                ax2.tick_params(axis='y', labelcolor='red', labelsize=7)\n",
    "            except:\n",
    "                pass  # Skip KDE if it fails\n",
    "        \n",
    "        ax.set_title(f'{col}', fontsize=10, fontweight='bold')\n",
    "        ax.set_xlabel('Value', fontsize=8)\n",
    "        ax.set_ylabel('Frequency', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=7)\n",
    "        \n",
    "        # Add statistics text\n",
    "        stats_text = (f'Count: {len(data):,}\\n'\n",
    "                        f'Mean: {data.mean():.3f}\\n'\n",
    "                        f'Std: {data.std():.3f}\\n'\n",
    "                        f'Min: {data.min():.3f}\\n'\n",
    "                        f'Max: {data.max():.3f}')\n",
    "        ax.text(0.98, 0.98, stats_text, transform=ax.transAxes, \n",
    "                fontsize=7, verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for i in range(n_cols, n_rows * cols_per_row):\n",
    "        row = i // cols_per_row\n",
    "        col_idx = i % cols_per_row\n",
    "        axes[row, col_idx].set_visible(False)\n",
    "\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.suptitle(f'Histogram Plots - All {n_cols} Numeric Columns', \n",
    "                    fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(f'{save_path}_histograms.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Histogram plots saved to {save_path}_histograms.png\")\n",
    "\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"âœ… Created histogram plots for {n_cols} columns\")\n",
    "\n",
    "\n",
    "def analyze_all_columns(df, save_path=None, show_plots=True):\n",
    "    \"\"\"\n",
    "    Convenience function to create both time series and histogram plots.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe to analyze\n",
    "    save_path : str, optional\n",
    "        Base path to save the plots (without extension)\n",
    "    show_plots : bool\n",
    "        Whether to display the plots\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"ðŸ“Š ANALYZING ALL COLUMNS IN DATAFRAME\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(f\"Numeric columns: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    print()\n",
    "\n",
    "    # Create time series plots\n",
    "    print(\"ðŸ”„ Creating time series plots...\")\n",
    "    plot_all_time_series(df, save_path=save_path, show_plots=show_plots)\n",
    "    print()\n",
    "\n",
    "    # Create histogram plots  \n",
    "    print(\"ðŸ”„ Creating histogram plots...\")\n",
    "    plot_all_histograms(df, save_path=save_path, show_plots=show_plots)\n",
    "    print()\n",
    "\n",
    "    print(\"âœ… Analysis complete!\")\n",
    "\n",
    "def create_dummies(df, columns):\n",
    "    \"\"\"\n",
    "    Create dummy variables for specified categorical columns\n",
    "    \n",
    "    Args:\n",
    "        df: pandas.DataFrame - Input DataFrame\n",
    "        columns: list - List of column names to create dummies for\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with dummy variables added and original categorical columns dropped\n",
    "    \"\"\"\n",
    "    df_dummies = df.copy()\n",
    "    \n",
    "    # Validate that all specified columns exist in the DataFrame\n",
    "    missing_columns = [col for col in columns if col not in df_dummies.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Columns not found in DataFrame: {missing_columns}\")\n",
    "    \n",
    "    print(f\"ðŸ”¢ Creating dummy variables for {len(columns)} columns: {columns}\")\n",
    "    \n",
    "    # Create dummies for each specified column\n",
    "    for col in columns:\n",
    "        # Get dummies with column name prefix to avoid naming conflicts\n",
    "        dummies = pd.get_dummies(df_dummies[col], prefix=col, drop_first=True)\n",
    "        \n",
    "        # Add dummy columns to DataFrame\n",
    "        df_dummies = pd.concat([df_dummies, dummies], axis=1)\n",
    "        \n",
    "        # Drop the original categorical column\n",
    "        df_dummies = df_dummies.drop(columns=[col])\n",
    "        \n",
    "        print(f\"  âœ“ Created {len(dummies.columns)} dummy variables for '{col}'\")\n",
    "    \n",
    "    print(f\"ðŸ“Š Final shape: {df_dummies.shape} (added {df_dummies.shape[1] - df.shape[1]} columns)\")\n",
    "    \n",
    "    return df_dummies\n",
    "\n",
    "\n",
    "print(\"âœ… Core functions defined successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
